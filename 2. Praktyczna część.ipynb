{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "## Dane \n",
    "\n",
    "Dane jak zostało wspomniane muszą być w formie `np.array` dla której `shape = (n_obserwacji, n_cech)`.\n",
    "\n",
    "Często cech jest bardzo dużo (np. w analizie tekstu albo w danych genomowych), przy czym dużo z nich jest równe zero - wtedy warto stosować rzadkie macierze `scipy.sparse`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W scikit-learn dodane są pewne typowe zbiory danych, np. zestaw informacji na temat cen bostońskich mieszkań.\n",
    "Jeśli korzystacie ze środowiska, tak, jak instalowalismy je ostatnio, zbiór danych powinien znajdować się w pliku wskazywanym przez podobnie wyglądającą ścieżkę: \n",
    "`/home/piotrek/miniconda3/envs/BioML/lib/python3.6/site-packages/sklearn/datasets/data/boston_house_prices.csv`\n",
    "Zobaczmy jak wyglądają te dane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Tu skorzystamy z możliwości, jakie daje jupyter notebook - można wywoływać bashowe polecenia poprzedzając je znakiem wykrzyknika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/piotrek/miniconda3/envs/BioML/lib/python3.6/site-packages/sklearn/datasets/data/boston_house_prices.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimportujmy dane dzięki funkcji read_csv z pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "# read_csv?\n",
    "dane_boston = read_csv(\"/home/piotrek/miniconda3/envs/BioML/lib/python3.6/site-packages/sklearn/datasets/data/boston_house_prices.csv\", skiprows=[0])\n",
    "display(dane_boston.head())\n",
    "print(dane_boston.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względu na to, że jest to popularny zbiór danych, w scikit-learn jest specjalna funkcja wczytująca te dane wraz z metadanymi opisującymi, co się w tym zbiorze znajduje. Skorzystajmy z niej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "boston = sklearn.datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokażmy zawartość wczytanego obiektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wymiary danych Boston [n_obserwacji, n_cech]: \", boston.data.shape)\n",
    "display(boston.data[0])\n",
    "print(\"Wymiary zestawu testowego Boston: \", boston.target.shape)\n",
    "display(boston.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy, czy zbiór zawiera NaN (not a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.isnan(boston.data).any())\n",
    "print(np.isnan(boston.target).any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiemy już jak wczytywać dane, teraz podzielmy je na zbiór treningowy i testowy.\n",
    "## Zbiór testowy i treningowy\n",
    "\n",
    "<img src=\"figures/train_test_split_matrix.svg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#train_test_split?\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(boston.data, boston.target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak przygotowane dane będziemy mogli następnie przekazywać estymatorom z scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja - ordinary least squares regression\n",
    "\n",
    "Mając zestaw danych (w różny sposób zaszumionych), znaleźć liniową zależność, która najlepiej pozwala odnajdować poszukiwaną wartość.\n",
    "\n",
    "<img src=\"figures/ols.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "Trzeba zdefiniować najlepsze dopasowanie. Zdefiniujemy je jako najmniejszy błąd kwadratowy.\n",
    "\n",
    "\n",
    "$$\n",
    "SE = \\sum_{i=1}^{n} r_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "r = y_i - f(x_i, \\beta)\n",
    "$$\n",
    "\n",
    "$y$ to faktyczna wartość funkcji, $f(x_i, \\beta)$ to przewidywana przez naszą regresję wartość funkcji w danym miejscu\n",
    "\n",
    "<img src=\"figures/varianceexplained.gif\" width=\"80%\">\n",
    "\n",
    "(source: http://my.ilstu.edu/~wjschne/138/Psychology138Lab9.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy, jak to wygląda na syntetycznym przykładzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy niezbędne biblioteki\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy chcieli zamodelować funkcję $y = 2x + 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100) # 100 liczb od 0 do 10\n",
    "y = 2 * x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 8) # zmiana rozmiaru wykresu (standardowo jest mały)\n",
    "plt.rcParams.update({'font.size': 17}) # zmiana rozmiaru czcionki\n",
    "plt.plot(x, y, 'o') # wyrysowanie jako kółka danych x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak wspomniano wcześniej, scikit-learn operuje na macierzach o rozmiarze `[n_obserwacji, n_cech]`\n",
    "Obserwacji mamy sto, cecha jest jedna (wartość x w punkcie). Musimy zmienić wymiary macierzy, żeby scikit-learn nas zrozumiał.\n",
    "<img src=\"figures/train_test_split_matrix.svg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Na początku wymiary: ', x.shape)\n",
    "print(x)\n",
    "X = x[:, np.newaxis]\n",
    "print('Po dodaniu wymiaru: ', X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzielimy nasz zbiór na dane treningowe i testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X_train, y_train, 'bo')\n",
    "plt.plot(X_test, y_test, 'rx')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosujmy model regresji liniowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "fig = plt.figure()\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
    "plt.plot(X_train, y_train, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udało nam się odkryć funkcję, która generowała dane, ale to było proste - funkcja była bardzo łatwa, nie było żadnego szumu w danych.\n",
    "\n",
    "Dodajmy **szum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "min_pt = 2 * X.min() + 5\n",
    "max_pt = 2 * X.max() + 5\n",
    "fig = plt.figure()\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "\n",
    "min_pt = 2 * X.min() + 5\n",
    "max_pt = 2 * X.max() + 5\n",
    "\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')\n",
    "ax = plt.figure()\n",
    "min_pt_pred = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt_pred = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "plt.plot([X.min(), X.max()], [min_pt_pred, max_pt_pred], 'g--')\n",
    "plt.plot(X_train, y_train, 'bo')\n",
    "plt.plot(X_test, y_test, 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn odkrył niemal dokładnie funkcję, która generowała dane, pomimo szumu. Zobaczmy jednak, co działoby się, gdybyśmy mieli **mniej danych treningowych**. Zrobimy to korzystając z interaktywnych widgetów. Żeby móc z nich skorzystać musimy całą naszą procedurę przemienić w funkcję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dopasuj_regresje(rozmiar_danych):\n",
    "    plt.figure()\n",
    "    x = np.linspace(0, 10, rozmiar_danych)\n",
    "    X = x[:, np.newaxis]\n",
    "    rng = np.random.RandomState(42)\n",
    "    y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "    print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "\n",
    "    min_pt = 2 * X.min() + 5\n",
    "    max_pt = 2 * X.max() + 5\n",
    "\n",
    "\n",
    "    plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')\n",
    "\n",
    "    min_pt_pred = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "    max_pt_pred = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "    plt.plot([X.min(), X.max()], [min_pt_pred, max_pt_pred], 'g--')\n",
    "    plt.plot(X_train, y_train, 'bo')\n",
    "    plt.plot(X_test, y_test, 'rx')\n",
    "    return regressor.coef_, regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "widgets.interact(dopasuj_regresje, rozmiar_danych=widgets.BoundedIntText(min=4, max=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbyt mało danych - duży szum - dopasowania zmieniają się mocno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daj_wspolczynniki(rozmiar_danych):\n",
    "    x = np.linspace(0, 10, rozmiar_danych)\n",
    "    X = x[:, np.newaxis]\n",
    "    rng = np.random.RandomState(42)\n",
    "    y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    return regressor.coef_, regressor.intercept_\n",
    "\n",
    "a = list()\n",
    "b = list()\n",
    "for i in range(4, 4000):\n",
    "    wspolczynniki = daj_wspolczynniki(i)\n",
    "    a.append(wspolczynniki[0])\n",
    "    b.append(wspolczynniki[1])\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(a)), a)\n",
    "plt.plot([0, len(a)], [2, 2])\n",
    "plt.title(\"Wahania dopasowanej wartości współczynnika przy x\")\n",
    "plt.xlabel(\"Liczebność zbioru danych\")\n",
    "plt.ylabel(\"Dopasowana wartość współczynnika\")\n",
    "plt.show()\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(b)), b)\n",
    "plt.plot([0, len(a)], [5, 5])\n",
    "plt.title(\"Wahania dopasowanej wartości przy przecięciu osi y\")\n",
    "plt.xlabel(\"Liczebność zbioru danych\")\n",
    "plt.ylabel(\"Dopasowana wartość\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A co jeśli funkcja jest inna? Być może nie da się jej zamodelować regresją liniową\n",
    "$$ y = \\sin(x) + szum$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "y = np.sin(x) + rng.uniform(-0.75, 0.75, size=len(x))\n",
    "fig = plt.figure()\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.title(r'$y = \\sin(x)$')\n",
    "X = x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], label=\"Dopasowana prosta regresyjna\")\n",
    "plt.plot(X_train, y_train, 'o', label=\"Dane treningowe\")\n",
    "\n",
    "predict_data = np.linspace(-5, 5, 20)\n",
    "predict_data = predict_data[:, np.newaxis]\n",
    "predicted_values = regressor.predict(predict_data)\n",
    "plt.plot(predict_data, predicted_values, 'gx', markersize=10, label=\"Przewidziane wartości\")\n",
    "plt.legend(loc=\"best\", prop={'size': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten model nie jest wystarczająco skomplikowany, by oddać naturę tych danych. Zastosujmy regresję K-Sąsiadów. W tej regresji patrzymy jaką wartość ma najbliższych K-sąsiadów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "kneighbor_regression = KNeighborsRegressor(n_neighbors=1)\n",
    "kneighbor_regression.fit(X_train, y_train)\n",
    "y_pred_train = kneighbor_regression.predict(X_train)\n",
    "plt.figure()\n",
    "plt.plot(X_train, y_train, 'o', label=\"wartość faktyczna\", markersize=10)\n",
    "plt.plot(X_train, y_pred_train, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "plt.legend(loc='best', prop={'size': 8});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = kneighbor_regression.predict(X_test)\n",
    "fig = plt.figure()\n",
    "plt.plot(X_test, y_test, 'o', label=\"wartość faktyczna\", markersize=8)\n",
    "plt.plot(X_test, y_pred_test, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.legend(loc='best', prop={'size': 8});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_neighbors_effect(K):\n",
    "    fig = plt.figure()\n",
    "    kneighbor_regression = KNeighborsRegressor(n_neighbors=K)\n",
    "    kneighbor_regression.fit(X_train, y_train)\n",
    "    y_pred_test = kneighbor_regression.predict(X_test)\n",
    "\n",
    "    plt.plot(X_test, y_test, 'o', label=\"wartość faktyczna\", markersize=8)\n",
    "    plt.plot(X_test, y_pred_test, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "    plt.plot(x, np.sin(x))\n",
    "    plt.title(\"Regresja K sąsiadów dla K = \" + str(K))\n",
    "    plt.legend(loc='best',prop={'size': 8})\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interactive(N_neighbors_effect, K=widgets.IntSlider(min=1, value=1,max=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trzeba dobrze dobrać złożoność modelu do problemu\n",
    "\n",
    "+ Zbyt złożony, wyczulony model zacznie uczyć się danych treningowych, będzie słabo generalizował\n",
    "+ Zbyt prosty, uśredniający model nie odda złożoności problemu\n",
    "\n",
    "<img src=\"figures/plot_kneigbors_regularization.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wracamy do modeli liniowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Musimy jakoś mierzyć jakość naszego modelu - do regresji często stosowana jest miara $R^2$\n",
    "\n",
    "\n",
    "<img src=\"figures/r_squared.png\" width=\"65%\">\n",
    "\n",
    "Można również mierzyć za pomocą MSE - Mean Squared Error:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{predicted}_i - \\text{true}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "x = np.linspace(0, 10, 100)\n",
    "X = x[:, np.newaxis]\n",
    "rng = np.random.RandomState(42)\n",
    "y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "Y = y[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "\n",
    "min_pt = 2 * X.min() + 5\n",
    "max_pt = 2 * X.max() + 5\n",
    "\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')\n",
    "min_pt_pred = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt_pred = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "plt.plot([X.min(), X.max()], [min_pt_pred, max_pt_pred], 'g--')\n",
    "plt.plot(X_train, y_train, 'bo')\n",
    "plt.plot(X_test, y_test, 'rx')\n",
    "\n",
    "plt.show()\n",
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS jest zgodnie z twierdzeniem Gaussa-Markov jest BLUE - Best Linear Unbiased Estimator.\n",
    "Ale możemy chcieć zrezygnować z braku zbiasowania na rzecz lepszego wyniku (ograniczając błąd pochodzący z wariancji).\n",
    "\n",
    "<img src=\"figures/bias-and-variance.jpg\" width=\"65%\">\n",
    "\n",
    "<img src=\"figures/biaserror.png\" width=\"65%\">\n",
    "\n",
    "## Regularyzacja\n",
    "\n",
    "Dla regresji liniowej minimalizowaliśmy coś takiego: \n",
    "\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n",
    "\n",
    "<img src=\"figures/regularization.png\" width=\"65%\">\n",
    "\n",
    "\n",
    "Dla Ridge regression będziemy minimalizować: \n",
    "\n",
    "$$ \\text{min}_{w,b}  \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_2^2$$ \n",
    "\n",
    "Dla Lasso regression: \n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i \\frac{1}{2} || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_1$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W scikit-learn te regresje także są dostępne:\n",
    "\n",
    "`from sklearn.linear_model import Ridge`\n",
    "\n",
    "`ridge = Ridge(alpha=alpha).fit(X_train, y_train)`\n",
    "\n",
    "`from sklearn.linear_model import Lasso`\n",
    "\n",
    "`lasso = Lasso(alpha=alpha).fit(X_train, y_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "# Klastrowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy trzy skupiska punktów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy, by algotyrm klastrowania przyporządkował punkty do klastrów - my je widzimy, ale komputer musi na jakiejś zasadzie je pogrupować. W metodzie K-Means podajemy algorytmowi liczbę klastrów, które do których ma pokwalifikować punkty, a następnie algorytm znajduje takie przyporządkowanie.\n",
    "Bazujemy tu na odległości euklidesowej (czyli zwykłej odległości)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten przykład dla nas jest oczywisty, ale w przypadku wielowymiarowych danych nasz wzrok i wyobraźnia zawodzą - a te metody komputerowe można przenieść na wyższe wymiary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czasem nie chcemy klastrować według odległości\n",
    "Wtedy lepsze mogą okazać się metody bazujące na zagęszczeniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=400,\n",
    "                  noise=0.1,\n",
    "                  random_state=1)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.2,\n",
    "            min_samples=10,\n",
    "            metric='euclidean')\n",
    "prediction = db.fit_predict(X)\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jeszcze jeden przykład, w którym wolimy klastrowanie według zagęszczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1500, \n",
    "                    factor=.4, \n",
    "                    noise=.05)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "X, y = make_circles(n_samples=1500, \n",
    "                    factor=.4, \n",
    "                    noise=.05)\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "plt.figure()\n",
    "plt.title(\"KMeans\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km.fit_predict(X))\n",
    "\n",
    "db = DBSCAN(eps=0.2)\n",
    "plt.figure()\n",
    "plt.title(\"DBSCAN - Density-based Spatial Clustering of Applications with Noise\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=db.fit_predict(X));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Część przykładów z **SciPy2017 Scikit-learn Tutorial**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
