{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Najprostszy przykład uczenia - regresja\n",
    "\n",
    "Chcemy dopasować prostą tak, by błąd kwadratowy był najmniejszy:\n",
    "\n",
    "$$\n",
    "SE = \\sum_{i=1}^{n} r_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "r = y_i - f(x_i, \\beta)\n",
    "$$\n",
    "\n",
    "$y$ to faktyczna wartość funkcji, $f(x_i, \\beta)$ to przewidywana przez naszą regresję wartość funkcji w danym miejscu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy niezbędne biblioteki\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy chcieli zamodelować funkcję $y = 2x + 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100) # 100 liczb od 0 do 10\n",
    "y = 2 * x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 8) # zmiana rozmiaru wykresu (standardowo jest mały)\n",
    "plt.rcParams.update({'font.size': 17}) # zmiana rozmiaru czcionki\n",
    "plt.plot(x, y, 'o') # wyrysowanie jako kółka danych x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak wspomniano wcześniej, scikit-learn operuje na macierzach o rozmiarze `[n_obserwacji, n_cech]`\n",
    "Obserwacji mamy sto, cecha jest jedna (wartość x w punkcie). Musimy zmienić wymiary macierzy, żeby scikit-learn nas zrozumiał.\n",
    "<img src=\"figures/train_test_split_matrix.svg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Na początku wymiary: ', x.shape)\n",
    "print(x)\n",
    "X = x[:, np.newaxis]\n",
    "print('Po dodaniu wymiaru: ', X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importujemy potrzebną bibliotekę z scikit-learn\n",
    "\n",
    "Dzielimy nasz zbiór na dane treningowe i testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train, y_train, 'bo')\n",
    "plt.plot(X_test, y_test, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosujmy model regresji liniowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
    "plt.plot(X_train, y_train, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udało nam się odkryć funkcję, która generowała dane, ale to było proste - funkcja była bardzo łatwa, nie było żadnego szumu w danych.\n",
    "\n",
    "Dodajmy **szum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "min_pt = 2 * X.min() + 5\n",
    "max_pt = 2 * X.max() + 5\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "\n",
    "min_pt = 2 * X.min() + 5\n",
    "max_pt = 2 * X.max() + 5\n",
    "\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')\n",
    "\n",
    "min_pt_pred = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt_pred = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "plt.plot([X.min(), X.max()], [min_pt_pred, max_pt_pred], 'g--')\n",
    "plt.plot(X_train, y_train, 'bo')\n",
    "plt.plot(X_test, y_test, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn odkrył niemal dokładnie funkcję, która generowała dane, pomimo szumu. Zobaczmy jednak, co działoby się, gdybyśmy mieli **mniej danych treningowych**. Zrobimy to korzystając z interaktywnych widgetów. Żeby móc z nich skorzystać musimy całą naszą procedurę przemienić w funkcję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dopasuj_regresje(rozmiar_danych):\n",
    "    x = np.linspace(0, 10, rozmiar_danych)\n",
    "    X = x[:, np.newaxis]\n",
    "    rng = np.random.RandomState(42)\n",
    "    y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "    print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "\n",
    "    min_pt = 2 * X.min() + 5\n",
    "    max_pt = 2 * X.max() + 5\n",
    "\n",
    "\n",
    "    plt.plot([X.min(), X.max()], [min_pt, max_pt], 'black')\n",
    "\n",
    "    min_pt_pred = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "    max_pt_pred = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "    plt.plot([X.min(), X.max()], [min_pt_pred, max_pt_pred], 'g--')\n",
    "    plt.plot(X_train, y_train, 'bo')\n",
    "    plt.plot(X_test, y_test, 'rx')\n",
    "    return regressor.coef_, regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "widgets.interact(dopasuj_regresje, rozmiar_danych=widgets.BoundedIntText(min=4, max=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbyt mało danych - duży szum - dopasowania zmieniają się mocno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daj_wspolczynniki(rozmiar_danych):\n",
    "    x = np.linspace(0, 10, rozmiar_danych)\n",
    "    X = x[:, np.newaxis]\n",
    "    rng = np.random.RandomState(42)\n",
    "    y = 2 * x + 5 + rng.uniform(-3, 3, size=len(x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    return regressor.coef_, regressor.intercept_\n",
    "\n",
    "a = list()\n",
    "b = list()\n",
    "for i in range(4, 4000):\n",
    "    wspolczynniki = daj_wspolczynniki(i)\n",
    "    a.append(wspolczynniki[0])\n",
    "    b.append(wspolczynniki[1])\n",
    "plt.plot(np.arange(len(a)), a)\n",
    "plt.plot([0, len(a)], [2, 2])\n",
    "plt.title(\"Wahania dopasowanej wartości współczynnika przy x\")\n",
    "plt.xlabel(\"Liczebność zbioru danych\")\n",
    "plt.ylabel(\"Dopasowana wartość współczynnika\")\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(b)), b)\n",
    "plt.plot([0, len(a)], [5, 5])\n",
    "plt.title(\"Wahania dopasowanej wartości przy przecięciu osi y\")\n",
    "plt.xlabel(\"Liczebność zbioru danych\")\n",
    "plt.ylabel(\"Dopasowana wartość\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A co jeśli funkcja jest inna? Być może nie da się jej zamodelować regresją liniową\n",
    "$$ y = \\sin(x) + szum$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 200)\n",
    "y = np.sin(x) + rng.uniform(-0.75, 0.75, size=len(x))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.title(r'$y = \\sin(x)$')\n",
    "X = x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print('Waga współczynnika przy x: ', regressor.coef_)\n",
    "print('Punkt przecięcia osi y: ', regressor.intercept_)\n",
    "min_pt = X.min() * regressor.coef_[0] + regressor.intercept_\n",
    "max_pt = X.max() * regressor.coef_[0] + regressor.intercept_\n",
    "\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt])\n",
    "plt.plot(X_train, y_train, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten model nie jest wystarczająco skomplikowany, by oddać naturę tych danych. Zastosujmy regresję K-Sąsiadów. W tej regresji patrzymy jaką wartość ma najbliższych K-sąsiadów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "kneighbor_regression = KNeighborsRegressor(n_neighbors=1)\n",
    "kneighbor_regression.fit(X_train, y_train)\n",
    "y_pred_train = kneighbor_regression.predict(X_train)\n",
    "\n",
    "plt.plot(X_train, y_train, 'o', label=\"wartość faktyczna\", markersize=10)\n",
    "plt.plot(X_train, y_pred_train, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = kneighbor_regression.predict(X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, 'o', label=\"wartość faktyczna\", markersize=8)\n",
    "plt.plot(X_test, y_pred_test, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbor_regression = KNeighborsRegressor(n_neighbors=15)\n",
    "kneighbor_regression.fit(X_train, y_train)\n",
    "y_pred_test = kneighbor_regression.predict(X_test)\n",
    "\n",
    "plt.plot(X_test, y_test, 'o', label=\"wartość faktyczna\", markersize=8)\n",
    "plt.plot(X_test, y_pred_test, 's', label=\"wartość przewidywana\", markersize=4)\n",
    "plt.plot(x, np.sin(x))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trzeba dobrze dobrać złożoność modelu do problemu\n",
    "\n",
    "+ Zbyt złożony, wyczulony model zacznie uczyć się danych treningowych, będzie słabo generalizował\n",
    "+ Zbyt prosty, uśredniający model nie odda złożoności problemu\n",
    "\n",
    "<img src=\"figures/plot_kneigbors_regularization.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krótko o klastrowaniu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy trzy skupiska punktów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy, by algotyrm klastrowania przyporządkował punkty do klastrów - my je widzimy, ale komputer musi na jakiejś zasadzie je pogrupować. W metodzie K-Means podajemy algorytmowi liczbę klastrów, które do których ma pokwalifikować punkty, a następnie algorytm znajduje takie przyporządkowanie.\n",
    "Bazujemy tu na odległości euklidesowej (czyli zwykłej odległości)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten przykład dla nas jest oczywisty, ale w przypadku wielowymiarowych danych nasz wzrok i wyobraźnia zawodzą - a te metody komputerowe można przenieść na wyższe wymiary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czasem nie chcemy klastrować według odległości\n",
    "Wtedy lepsze mogą okazać się metody bazujące na zagęszczeniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=400,\n",
    "                  noise=0.1,\n",
    "                  random_state=1)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.2,\n",
    "            min_samples=10,\n",
    "            metric='euclidean')\n",
    "prediction = db.fit_predict(X)\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jeszcze jeden przykład, w którym wolimy klastrowanie według zagęszczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1500, \n",
    "                    factor=.4, \n",
    "                    noise=.05)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "X, y = make_circles(n_samples=1500, \n",
    "                    factor=.4, \n",
    "                    noise=.05)\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "plt.figure()\n",
    "plt.title(\"KMeans\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km.fit_predict(X))\n",
    "\n",
    "db = DBSCAN(eps=0.2)\n",
    "plt.figure()\n",
    "plt.title(\"DBSCAN - Density-based Spatial Clustering of Applications with Noise\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=db.fit_predict(X));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drzewa decyzyjne - krótka ilustracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures.plot_interactive_tree import *\n",
    "plot_tree_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Część przykładów z **SciPy2017 Scikit-learn Tutorial**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
